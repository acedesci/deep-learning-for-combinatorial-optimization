import tensorflow as tf
from tensorflow.contrib.rnn import LSTMCell, GRUCell, MultiRNNCell, DropoutWrapper
from tqdm import tqdm


from tensorflow.contrib import seq2seq
import random
import numpy as np

import matplotlib.pyplot as plt

##############################################################
### CLASSIFICATION OF MELODIC SEQUENCE (HUMAN GENERATED ?) ###
##############################################################


"""
- Convolution on input melody sequence [batch size, seq length, # notes]
- True label (sequence generated by TrueLabelGenerator), False label (random sequence)
"""


class DiscrimatorModel():

    def __init__(self, args):
        self.batch_size = args['batch_size']
        self.max_length =args['max_length'] # input sequence length
        self.num_roots=args['num_roots'] # dimension of a melody
        self.num_classes=args['num_classes'] # number of output classes 
        self.melody_embed=args['melody_embed']
        self.filter_sizes=args['filter_sizes']
        self.num_filters=args['num_filters']

        self.initializer = tf.random_uniform_initializer(-args['init_range'], args['init_range'])

        # build the model
        self.build_model()

    def build_model(self):
        # Placeholders for our input data and target
        self.data= tf.placeholder(tf.float32, [None, self.max_length, self.num_roots]) # tensor block holding the input sequences [Batch Size, Sequence Length, Features]
        self.target = tf.placeholder(tf.float32, [None, 2]) # tensor block holding the target labels (0 or 1)
        self.dropout_keep_prob = 0.9 #tf.placeholder(tf.float32, name="dropout_keep_prob")
        
        # Embed melody sequence
        self.W_embed = tf.Variable(tf.truncated_normal([1,self.num_roots,self.melody_embed]), name="W_ref")
        self.embedded_melody = tf.nn.conv1d(self.data, self.W_embed, 1, "VALID", name="melody")
        self.embedded_melody_expanded = tf.expand_dims(self.embedded_melody, -1)
        """
        TODO: DO CONVOLUTION, MAX POOLING, FFN here to output self.prediction
        """
        pooled_outputs = []
        for i, filter_size in enumerate(self.filter_sizes):
            with tf.name_scope("conv-maxpool-%s" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, self.melody_embed, 1, self.num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name="W")
                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name="b")
                conv = tf.nn.conv2d(
                    self.embedded_melody_expanded,
                    W,
                    strides=[1, 1, 1, 1],
                    padding="VALID",
                    name="conv")
                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")
                # Max-pooling over the outputs
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, self.max_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name="pool")
                pooled_outputs.append(pooled)
 
        # Combine all the pooled features
        num_filters_total = self.num_filters * len(self.filter_sizes)
        self.h_pool = tf.concat(pooled_outputs, 3)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])

        # Add dropout
        #with tf.name_scope("dropout"):
            #self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)
        self.h_drop = self.h_pool_flat

        # Predictions
        with tf.name_scope("output"):
            W = tf.Variable(tf.truncated_normal([num_filters_total, self.num_classes], stddev=0.1), name="W")
            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name="b")
            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name="scores")
            self.predictions = tf.argmax(self.scores, 1, name="predictions")

        self.predictions=tf.stack([self.predictions,1-self.predictions],1)

        # Loss function
        self.cross_entropy = -tf.reduce_sum(self.target * tf.log(tf.to_float(tf.clip_by_value(self.predictions,0,1)))+0.00000000001)
        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.clip_by_value(self.scores,0,1), labels=self.target))
        # Note to myself : I changed 1e-10 to 0 for datatype constraint purposes

        # Optimizer and minimize operation
        self.optimizer = tf.train.AdamOptimizer()
        self.minimize = self.optimizer.minimize(self.loss)


        # Accuracy
        self.mistakes = tf.not_equal(tf.argmax(self.target, 1), tf.argmax(self.predictions, 1))
        self.error = tf.reduce_mean(tf.cast(self.mistakes, tf.float32))

    def train(self,sess,inp,out):
        feed={self.data: inp, self.target: out}
        sess.run(self.minimize,feed_dict=feed)
        self.trained=True

    def test(self,sess,test_input,test_output):
        feed={self.data: test_input, self.target: test_output}
        accuracy = sess.run(self.error,feed_dict=feed)
        return accuracy



def main(train_input,train_output,test_input,test_output):

    print("Initializing the Model...")

    args={}

    args['batch_size']=3
    args['max_length']=8
    args['num_roots']=12
    args['num_classes']=2 # TRUE and FALSE
    args['melody_embed']=9
    args['filter_sizes']=[1,2,3]
    args['num_filters']=1

    args['init_range']=1

    model = DiscrimatorModel(args)

    print("Starting training...")
    with tf.Session() as sess:
        # init variables
        tf.global_variables_initializer().run()
        saver = tf.train.Saver(tf.global_variables())

        batch_size=args['batch_size']
        epoch=100

        for i in tqdm(range(epoch)): # epoch i
            ptr = 0
            no_of_batches = int(len(train_input)/batch_size)

            for j in range(no_of_batches):
                inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]
                #print '\n input: \n',inp
                #print '\n output: \n',out
                model.train(sess,inp,out)
                ptr+=batch_size

            if i % 100 == 0 and not(i == 0):
                saver.save(sess,"save/" +str(i) +".ckpt")

        #print 'X:',sess.run(model.target,feed_dict={model.data: inp, model.target: out})
        #print 'X:',sess.run(model.predictions,feed_dict={model.data: inp, model.target: out})

        incorrect = model.test(sess,inp,out)
        print 'Error:',incorrect
        #print('Epoch {:2d} error {:3.1f}%'.format(i + 1, 100 * incorrect))

        print("Training is COMPLETE!")
        #saver.save(sess,"save/model.ckpt")




if __name__ == "__main__":

    import data_loader
    import random
    np.random.seed(123)

    ######################
    ### CREATE DATASET ###
    ######################

    true_dataset = data_loader.TrueDataGenerator(max_step=8)
    true_dataset.process_maj()
    train_input=true_dataset.training_seq
    train_output=true_dataset.target_seq


    false_dataset = data_loader.FalseDataGenerator(max_step=8)
    false_dataset.build_fake_data(N=len(train_input))
    for sequence in false_dataset.training_seq:
        train_input.append(sequence)
    for label in false_dataset.target_seq:
        train_output.append(label)


    # Shuffle data
    c = list(zip(train_input, train_output))
    random.shuffle(c)
    train_input, train_output = zip(*c)

    # Cross validation
    NUM_EXAMPLES = 10000
    test_input = train_input[NUM_EXAMPLES:]
    test_output = train_output[NUM_EXAMPLES:] #everything beyond 10,000
     
    train_input = train_input[:NUM_EXAMPLES]
    train_output = train_output[:NUM_EXAMPLES] #till 10,000

    print(train_input[0],train_output[0])

    #############################
    ### BUILD & EXECUTE GRAPH ###
    #############################

    main(train_input,train_output,test_input,test_output)